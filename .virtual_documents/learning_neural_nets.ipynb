import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import torch
import util.utilities

import learning

from importlib import reload


from sklearn.datasets import make_circles, make_blobs


reload(util.utilities)





torch.manual_seed(42)

# Simple -5 to 5 range with 100 steps
X = torch.linspace(-5, 5, 100)
# Create a simple sine wave as our target for prediction
y = torch.sin(X)

# Datasets need to be in the form of 1 array per dataset. Since we only have 1 value per, just unsqueeze.
# Unsqueeze takes single values [1,2,3] and makes them have 1 row each [[1], [2]. [3]], which is what we need
dataset = list(zip(X.unsqueeze(1), y.unsqueeze(1)))

# A simple model
model = torch.nn.Sequential(
    torch.nn.Linear(1, 1000),
    torch.nn.ReLU(),
    torch.nn.Linear(1000, 1)
).to("cuda")

# More complex, harder to tune learn rate, etc. Probably would end up better with more tweaks?
# model = torch.nn.Sequential(
#     torch.nn.Linear(1, 100),
#     torch.nn.ReLU(),
#     torch.nn.Linear(100, 100),
#     torch.nn.ReLU(),
#     torch.nn.Linear(100, 100),
#     torch.nn.ReLU(),
#     torch.nn.Linear(100, 100),
#     torch.nn.ReLU(),
#     torch.nn.Linear(100, 1),
# ).to("cuda")

epochs = 30

optimizer = torch.optim.SGD(model.parameters(), lr=0.001)
# Higher learning rates better for more complex model
# optimizer = torch.optim.SGD(model.parameters(), lr=0.08)
# scheduler = torch.optim.lr_scheduler.OneCycleLR(
#     optimizer, max_lr=0.04, steps_per_epoch=len(dataset), epochs=epochs
# )
scheduler = None

# I wrote this learner. Learners are very abstractable. In nutshell, all it does is:
# For each batch, feed it into the model to get preds. 
# Using a loss function, get the loss on the preds vs. the labels. 
# Adjust parameters/weights based on the gradient of loss func and learning rate 
#   (with, e.g. stochastic gradient descent, but others possible).
learner = learning.Learner(
    data_loaders=learning.DataLoaders(
        torch.utils.data.DataLoader(dataset, batch_size=10), 
        torch.utils.data.DataLoader(dataset, batch_size=10),
        torch.utils.data.DataLoader(dataset, batch_size=10)
    ),
    model=model,
    loss_function=torch.nn.MSELoss(),
    optimizer=optimizer,
    scheduler=scheduler,
    device="cuda"
    # device="cpu"
)
learner.train_model(epochs)
learner.plot_epoch_data()





with torch.inference_mode():
    y_preds = model(X.unsqueeze(1).to("cuda"))
    
print(f"mse: {torch.nn.MSELoss()(y_preds, y.unsqueeze(1).to('cuda'))}")
plt.scatter(x=X, y=y)
plt.scatter(x=X, y=y_preds.cpu())





# Simple classification dataset with 2 colors dots in 2 circles
circles = make_circles(n_samples=500, noise=0.026)
# x, y coords
circles_X = circles[0]
# Labels
circles_y = circles[1]

# Already a 2d array, don't need to unsqueeze X
X = torch.from_numpy(circles_X).type(torch.float)
y = torch.from_numpy(circles_y).type(torch.float).unsqueeze(1)

dataset = list(zip(X, y))

# A simple model
model = torch.nn.Sequential(
    torch.nn.Linear(2, 1000),
    torch.nn.ReLU(),
    torch.nn.Linear(1000, 1)
).to("cuda")

epochs = 20

optimizer = torch.optim.SGD(model.parameters(), lr=0.1)
scheduler = None

learner = learning.Learner(
    data_loaders=learning.DataLoaders(
        torch.utils.data.DataLoader(dataset, batch_size=10), 
        torch.utils.data.DataLoader(dataset, batch_size=10),
        torch.utils.data.DataLoader(dataset, batch_size=10)
    ),
    model=model,
    loss_function=torch.nn.BCEWithLogitsLoss(),
    optimizer=optimizer,
    scheduler=scheduler,
    device="cuda"
    # device="cpu"
)
learner.train_model(epochs)
learner.plot_epoch_data()


util.utilities.plot_decision_boundary(model, X, y)





# Simple classification dataset with 4 colors of dots in clusters
num_classes = 4
# Just the dimensionality (x, y) coords, so 2 dims
num_features = 2
blobs = make_blobs(
    n_samples=1000, 
    n_features=num_features, 
    centers=num_classes, 
    cluster_std=1.5, 
    random_state=42)
# x, y coords
blobs_X = blobs[0]
# Labels
blobs_y = blobs[1]

# Already a 2d array, don't need to unsqueeze X
X = torch.from_numpy(blobs_X).type(torch.float)
# Since we are outputting 4 classes, we need to one-hot encode the label integer
y = torch.nn.functional.one_hot(
    # One hot requires an int64
    torch.from_numpy(blobs_y).type(torch.int64),
    num_classes=num_classes
# But output requires a float
).type(torch.float)


dataset = list(zip(X, y))

# A simple model
model = torch.nn.Sequential(
    torch.nn.Linear(num_features, 1000),
    torch.nn.ReLU(),
    torch.nn.Linear(1000, num_classes)
).to("cuda")

epochs = 5

optimizer = torch.optim.SGD(model.parameters(), lr=0.1)
scheduler = None

learner = learning.Learner(
    data_loaders=learning.DataLoaders(
        torch.utils.data.DataLoader(dataset, batch_size=10), 
        torch.utils.data.DataLoader(dataset, batch_size=10),
        torch.utils.data.DataLoader(dataset, batch_size=10)
    ),
    model=model,
    loss_function=torch.nn.CrossEntropyLoss(),
    optimizer=optimizer,
    scheduler=scheduler,
    device="cuda"
    # device="cpu"
)
learner.train_model(epochs)
learner.plot_epoch_data()


util.utilities.plot_decision_boundary(model, X, y)
